{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Attention_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu08XcE8MhGr",
        "colab_type": "text"
      },
      "source": [
        "# Week 13 Exercise - Transformers\n",
        "\n",
        "In this notebook, we will explore the creation of a Transformer Network for English to French translation.  Note that **Transformers are resource intensive and hard to train.** You will want to run these notebooks on a machine equipped with a GPU or on [Google Colab](http://colab.research.google.com).\n",
        "\n",
        "To begin, let's import a corpus of paired English and French text.  Additionally, we'll tokenize the words (i.e. create a dictionary for each vocabulary associating every word with an integer index).  There is no need to modify this cell, but have a look at what is contained in fr_to_ix (for example) and in enlines.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOTAeFFxMhGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "    device = \"cuda:0\" \n",
        "else:  \n",
        "    device = \"cpu\" \n",
        "\n",
        "with open('./french.txt', encoding=\"utf-8\") as file:\n",
        "    frvocab = file.read().lower()\n",
        "    frvocab = ''.join([i if ord(i) < 128 else ' ' for i in frvocab])\n",
        "    frlines = frvocab.split('\\n')\n",
        "frlines = [re.sub(r'[^\\w\\s\\']','',i).split() for i in frlines]\n",
        "frvocab = set(re.sub(r'[^\\w\\s\\']','',frvocab).replace('\\n',' ').split(' '))\n",
        "\n",
        "with open('./english.txt', encoding=\"utf-8\") as file:\n",
        "    envocab = file.read().lower()\n",
        "    envocab = ''.join([i if ord(i) < 128 else '' for i in envocab])\n",
        "    enlines = envocab.split('\\n')\n",
        "enlines = [re.sub(r'[^\\w\\s]','',i).split() for i in enlines]\n",
        "envocab = set(re.sub(r'[^\\w\\s]','',envocab).replace('\\n',' ').strip().split(' '))\n",
        "envocab.add('<pad>')\n",
        "envocab.add('<start>')\n",
        "envocab.add('<eos>')\n",
        "frvocab.add('<pad>')\n",
        "frvocab.add('<start>')\n",
        "frvocab.add('<eos>')\n",
        "fr_to_ix = {word: i for i, word in enumerate(frvocab)}\n",
        "en_to_ix = {word: i for i, word in enumerate(envocab)}\n",
        "ix_to_fr = {fr_to_ix[word]:word for word in frvocab}\n",
        "ix_to_en = {en_to_ix[word]:word for word in envocab}\n",
        "enmax = 0\n",
        "frmax = 16\n",
        "\n",
        "for i,w in enumerate(enlines):\n",
        "    temp = len(w)\n",
        "    if temp > enmax:\n",
        "        enmax = temp\n",
        "\n",
        "for i,w in enumerate(frlines):\n",
        "    temp = len(w)\n",
        "    if temp > frmax:\n",
        "        frmax = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyXh6rbpMhG3",
        "colab_type": "text"
      },
      "source": [
        "Next we'll create a handful of helper functions that do things like\n",
        " - Tokenize an english string, run it through the transformer producing predictions, then convert back to a french string\n",
        " - Compare predicted and target output\n",
        " - Mask a string\n",
        " - Load paired english/french sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGC3fMYpMhG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    # Read in an english string\n",
        "    line = re.sub(r'[^\\w\\s]','',sentence).split()\n",
        "    # tokenize/pad for consistent sequence length\n",
        "    line = F.pad(torch.tensor([en_to_ix[w.lower()] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).unsqueeze(0).to(device)\n",
        "    # Create an array to hold the French sentence\n",
        "    target = torch.Tensor(1,frmax-1)\n",
        "    target = target.new_full((1,frmax-1),fr_to_ix['<pad>']).long().to(device)\n",
        "    # Start sentence with a <start> character\n",
        "    target[0,0] = fr_to_ix['<start>']\n",
        "    \n",
        "    src,trg = mask(line,target)\n",
        "    encoding = model.encode(line,src)\n",
        "    K,V = model.create_dec_KV(encoding)\n",
        "    for i in range(1,frmax-1):\n",
        "        test2 = model.decode(target,K,V,src,trg)\n",
        "        lastout = test2[0,i-1].argmax()\n",
        "        if lastout.item() == fr_to_ix['<eos>']:\n",
        "            break\n",
        "        target[0,i] = lastout\n",
        "        src,trg = mask(line,target)\n",
        "    translation = test2.argmax(2).squeeze(0)\n",
        "    translation_string = ''\n",
        "    for w in translation:\n",
        "        if ix_to_fr[w.item()] == '<eos>':\n",
        "            break\n",
        "        translation_string += ix_to_fr[w.item()] + ' '\n",
        "    return translation_string.strip()\n",
        "\n",
        "def compareoutput(preds,targetlist,loc=None):\n",
        "    # Compare model predictions with true translation\n",
        "    if loc is None:\n",
        "        loc = np.random.randint(len(preds))\n",
        "    predstr = ''\n",
        "    labelstr = ''\n",
        "    for i in range(len(preds[loc][0])):\n",
        "        if ix_to_fr[targetlist[loc][i+1].item()] == '<eos>':\n",
        "            break\n",
        "        predstr += ' '+ ix_to_fr[preds[loc][0][i].item()]\n",
        "        labelstr += ' ' + ix_to_fr[targetlist[loc][i+1].item()]\n",
        "    print(\"\\tOutput:\", predstr)\n",
        "    print(\"\\tTarget:\",labelstr)\n",
        "    \n",
        "class PositionalEncoder(nn.Module):\n",
        "    # Create a positional encoding generator\n",
        "    def __init__(self, d_model, max_seq_len = 58):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            for i in range(1,d_model,2):\n",
        "                pe[pos, i] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                \n",
        "        pe = pe\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:seq_len], \\\n",
        "        requires_grad=False).to(device)\n",
        "        return x\n",
        "\n",
        "def mask(input_seq,target_seq):\n",
        "    input_msk = (input_seq != en_to_ix['<pad>']).unsqueeze(1)\n",
        "    target_msk = (target_seq != fr_to_ix['<pad>']).unsqueeze(1)\n",
        "    size = target_seq.size(1) # get seq_len for matrix\n",
        "    nopeak_mask = np.triu(np.ones((1, size, size)),k=1)\n",
        "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask).to(device) == 0)\n",
        "    target_msk = target_msk & nopeak_mask\n",
        "    return input_msk,target_msk\n",
        "\n",
        "class custdata(Dataset):\n",
        "    # Create a custom dataset object to serve up paired english and french lines\n",
        "    def __init__(self,enlines,frlines):\n",
        "        self.data_len = len(enlines) \n",
        "        self.data = [F.pad(torch.tensor([en_to_ix[w] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).to(device) for line in enlines]\n",
        "        self.labels = []\n",
        "        for line in frlines:\n",
        "            line = ['<start>',*line,'<eos>']\n",
        "            self.labels.append(F.pad(torch.tensor([fr_to_ix[w] for w in line]),(0,frmax-len(line)),value = fr_to_ix['<pad>']).to(device))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i],self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEyqvSOrMhG-",
        "colab_type": "text"
      },
      "source": [
        "# Attention\n",
        "The first task is to code a self-attention mechanism, which corresponds to implementing Eq. 1 in Vaswani.\n",
        "#### http://jalammar.github.io/illustrated-transformer/ is a great reference for most of the programming in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_yiP_URMhG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class self_attention(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR Q, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.wk = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR K, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.wv = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR V, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scaler = np.sqrt(enc_dim)\n",
        "    \n",
        "    def QKV(self,x):\n",
        "        Q = self.wq(x)  #### TODO#### CALCULATE Q\n",
        "        K = self.wk(x)  #### TODO#### CALCULATE K\n",
        "        V = self.wv(x)  #### TODO#### CALCULATE V\n",
        "        return Q,K,V\n",
        "    \n",
        "    def score(self,Q,K,V,mask):\n",
        "        # scores are the stuff that goes inside the softmax\n",
        "        scores = Q @ K.T.permute(2,0,1)/self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = self.dropout(F.softmax(scores,-1)) \n",
        "        return scores @ V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
        "    \n",
        "    def forward(self,x,mask=None):\n",
        "        Q,K,V = self.QKV(x)\n",
        "        return self.score(Q,K,V,mask)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKsfsL6kMhHF",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to produce the \"special\" attention mechanism that takes keys and values from the encoder, but queries from the decoder.  This is very similar to the self-attention mechanism, except that there should be two inputs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8gxUI1MhHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encdec_attention(nn.Module):\n",
        "    def __init__(self,dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        self.wq = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
        "        self.wk = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
        "        self.wv = nn.Linear(dim, dim) #### TODO #### SAME AS ABOVE\n",
        "        self.scaler = np.sqrt(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def Q(self,x):\n",
        "        return self.wq(x)\n",
        "    \n",
        "    def score(self,Q,K,V,mask):\n",
        "        scores = Q @ K.T.permute(2,0,1)/self.scaler #### TODO #### SAME AS ABOVE\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        scores = self.dropout(F.softmax(scores,-1)) \n",
        "        return scores @ V  #### TODO #### SAME AS ABOVE\n",
        "    \n",
        "    def forward(self,x,K,V,mask):\n",
        "        # DB Note: I'm not sure that this signature is right.  Seems like we should be taking x from the\n",
        "        # decoder, as well as another argument (call it y?) from the encoder, then producing K,V,Q internally,\n",
        "        # just like in the self-attention scheme.  Otherwise, how are wk and wv being used here?  it looks like \n",
        "        # these parameters have been shifted over to the Transformer module's create_dec_KV method.\n",
        "        Q = self.Q(x)\n",
        "        out = self.score(Q,K,V,mask)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wte4ML9MhHM",
        "colab_type": "text"
      },
      "source": [
        "# Encoder and Decoder\n",
        "With the attention mechanisms coded, now we need to create encoder and decoder models. These correspond to the things inside the boxes in Figure 1 of Vaswani.  \n",
        "#### Fill in the forward passes of the encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xVVAVmMhHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.enc_dim = enc_dim\n",
        "        self.residual = nn.Linear(dim,enc_dim)\n",
        "        \n",
        "        self.attention = self_attention(dim,enc_dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(enc_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(enc_dim,enc_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(enc_dim)\n",
        "    \n",
        "    def forward(self,x,mask):  #### TODO #### SET UP FORWARD PASS OF ENCODER\n",
        "        residual = x\n",
        "        x = self.attention(x, mask)\n",
        "        if self.dim != self.enc_dim: ### DONT TOUCH, THIS IS TO HELP WITH THE RESIDUAL CONNECTION ###\n",
        "            x = self.residual(x)\n",
        "        x += residual\n",
        "        x = self.norm1(x)\n",
        "        residual = x\n",
        "        x = self.linear(x)\n",
        "        x += residual\n",
        "        return self.norm2(x)\n",
        "     \n",
        "        \n",
        "class decoder(nn.Module):\n",
        "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.attention = self_attention(input_size,dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.EDattention = encdec_attention(dim,dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "    \n",
        "    def forward(self,x,k,v,enc_mask,dec_mask):#### TODO #### SET UP FORWARD PASS OF DECODER\n",
        "        residual = x\n",
        "        x = self.attention(x, dec_mask)\n",
        "        x += residual\n",
        "        x = self.norm1(x)\n",
        "        residual = x\n",
        "        x = self.EDattention(x, k, v, enc_mask)\n",
        "        x += residual\n",
        "        x = self.norm2(x)\n",
        "        residual = x\n",
        "        x = self.linear(x)\n",
        "        x += residual\n",
        "        return self.norm3(x)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkDYmiYEMhHW",
        "colab_type": "text"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "Build the transformer itself by hooking together encoders and decoders.  Note the word embedding layers that we are going to learn.  \n",
        "\n",
        "#### Add encoders and decoders to transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTJWbpz-MhHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class transformer(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
        "        super().__init__()\n",
        "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
        "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
        "        \n",
        "        self.pe1 = PositionalEncoder(dim,enmax)\n",
        "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
        "        self.encoders = []\n",
        "    \n",
        "        for _ in range(2):\n",
        "            self.encoders.append(encoder(dim, encoder_dim, enc_vocab_size)) #### TODO #### ADD DESIRED # OF ENCODERS TO SELF.ENCODERS\n",
        "        \n",
        "        \n",
        "        self.encoders = nn.ModuleList(self.encoders)\n",
        "        \n",
        "        self.decoders = []\n",
        "        \n",
        "        for _ in range(2):\n",
        "            self.decoders.append(decoder(encoder_dim, encoder_dim, dec_vocab_size)) #### TODO #### ADD DESIRED # OF DECODERS TO SELF.DECODERS\n",
        "\n",
        "       \n",
        "        self.decoders = nn.ModuleList(self.decoders)\n",
        "        \n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(encoder_dim,dec_vocab_size),\n",
        "            nn.LogSoftmax(2)\n",
        "        )\n",
        "        \n",
        "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
        "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
        "    def create_dec_KV(self,z):\n",
        "        K = self.k(z)\n",
        "        V = self.v(z)\n",
        "        return K,V\n",
        "    \n",
        "    def encode(self,x,src):\n",
        "        x = self.embedding1(x)\n",
        "        x = self.pe1(x)\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x,src)\n",
        "        return x\n",
        "    \n",
        "    def decode(self,y,K,V,src,trg):\n",
        "        y = self.embedding2(y)\n",
        "        y = self.pe2(y)\n",
        "        for layer in self.decoders:\n",
        "            y = layer(y,K,V,src,trg)\n",
        "        return self.final(y)\n",
        "    \n",
        "    def forward(self,x,y,src,trg):\n",
        "        \n",
        "        x = self.encode(x,src)\n",
        "        K,V = self.create_dec_KV(x)\n",
        "        y = self.decode(y,K,V,src,trg)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxVT4o75MhHe",
        "colab_type": "text"
      },
      "source": [
        "# Train the network.\n",
        "\n",
        "##### This will be slow to train and require a lot of resources. You can reduce the batch_size to lower the vram requirement, you can reduce \n",
        "##### the run time by lowering number_of_lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHEFNhT6MhHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvGRNB4MhHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygNOoLTrMhHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUMBER_OF_LINES = 20000\n",
        "\n",
        "train = custdata(enlines[:NUMBER_OF_LINES],frlines[:NUMBER_OF_LINES])\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val = custdata(enlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000],frlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000])\n",
        "valloader = torch.utils.data.DataLoader(dataset=val, batch_size=1, shuffle=True, drop_last=False)\n",
        "test = custdata(enlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000],frlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000])\n",
        "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=1, shuffle=False, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9N5tYerMhH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad9015c1-d6d7-43f0-e3c4-c87c404995fa"
      },
      "source": [
        "for i in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for j,(context, target) in enumerate(trainloader):\n",
        "        trg_input = target[:,:-1]\n",
        "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
        "        targets = target[:,1:].contiguous().view(-1)\n",
        "        src,trg = mask(context,trg_input)\n",
        "        output = model(context,trg_input,src,trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    scheduler.step(total_loss)\n",
        "    print('Epoch:', i+1,' loss:', total_loss)\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    preds = []\n",
        "    targetlist = []\n",
        "    for j,(context, target) in enumerate(valloader):\n",
        "            trg_input = target[:,:-1]\n",
        "            targets = target.contiguous().view(-1)\n",
        "            targetlist.append(targets)\n",
        "            src,trg = mask(context,trg_input)\n",
        "            output = model(context,trg_input,src,trg)\n",
        "            pred = F.softmax(output,2).argmax(2)\n",
        "            preds.append(pred)\n",
        "            break\n",
        "    compareoutput(preds,targetlist,loc=0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  loss: 138.48340794444084\n",
            "\tOutput:  je es <eos> <eos>\n",
            "\tTarget:  tu te sens bien\n",
            "Epoch: 2  loss: 58.56694960594177\n",
            "\tOutput:  qui a <eos> <eos> voiture\n",
            "\tTarget:  qui a fait cette tarte\n",
            "Epoch: 3  loss: 45.46398398280144\n",
            "\tOutput:  tout le monde a cie\n",
            "\tTarget:  tout le monde t'appr cie\n",
            "Epoch: 4  loss: 36.215969547629356\n",
            "\tOutput:  j'adore livre n'a <eos> de\n",
            "\tTarget:  un enfant a besoin d'amour\n",
            "Epoch: 5  loss: 29.667451918125153\n",
            "\tOutput:  faites astu une animal\n",
            "\tTarget:  en avezvous un duplicata\n",
            "Epoch: 6  loss: 25.709640234708786\n",
            "\tOutput:  vous tes trop bruyante\n",
            "\tTarget:  vous tes trop maigrichons\n",
            "Epoch: 7  loss: 23.428112983703613\n",
            "\tOutput:  pourquoi n'est va de <eos>\n",
            "\tTarget:  pourquoi ne pas simplement rester\n",
            "Epoch: 8  loss: 21.884558267891407\n",
            "\tOutput:  appelle <eos> portes\n",
            "\tTarget:  rappelez vos chiens\n",
            "Epoch: 9  loss: 20.800335258245468\n",
            "\tOutput:  tesvous br g s\n",
            "\tTarget:  estu v g tarien\n",
            "Epoch: 10  loss: 19.990015506744385\n",
            "\tOutput:  tu es fort s amusant\n",
            "\tTarget:  tu es tr s marrante\n",
            "Epoch: 11  loss: 19.184503331780434\n",
            "\tOutput:  qui est au lit\n",
            "\tTarget:  qui est ce lit\n",
            "Epoch: 12  loss: 13.318163491785526\n",
            "\tOutput:  v a v <eos>\n",
            "\tTarget:  endosse ce ch que\n",
            "Epoch: 13  loss: 10.935527570545673\n",
            "\tOutput:  essaie nouveau de\n",
            "\tTarget:  allez essaie encore\n",
            "Epoch: 14  loss: 9.937721524387598\n",
            "\tOutput:  vous es d\n",
            "\tTarget:  tu es surmen\n",
            "Epoch: 15  loss: 9.277500312775373\n",
            "\tOutput:  tu tes libre <eos> aller\n",
            "\tTarget:  vous tes libre d'y aller\n",
            "Epoch: 16  loss: 8.845592446625233\n",
            "\tOutput:  tesvous catholique\n",
            "\tTarget:  estu productive\n",
            "Epoch: 17  loss: 8.471420999616385\n",
            "\tOutput:  ne fais quiconque pas\n",
            "\tTarget:  ne te fie personne\n",
            "Epoch: 18  loss: 8.145849578082561\n",
            "\tOutput:  tu avez l'air de\n",
            "\tTarget:  vous avez l'air resplendissantes\n",
            "Epoch: 19  loss: 7.882048912346363\n",
            "\tOutput:  vous avez fait ton r\n",
            "\tTarget:  vous avez fait votre part\n",
            "Epoch: 20  loss: 7.706877611577511\n",
            "\tOutput:  vous tes tr s timide\n",
            "\tTarget:  vous tes tr s timide\n",
            "Epoch: 21  loss: 7.538042761385441\n",
            "\tOutput:  ne fut pas j' t\n",
            "\tTarget:  ne jetez pas de pierres\n",
            "Epoch: 22  loss: 7.342203639447689\n",
            "\tOutput:  vous es d\n",
            "\tTarget:  tu es surmen\n",
            "Epoch: 23  loss: 6.449140854179859\n",
            "\tOutput:  aimestu que tu aimes les\n",
            "\tTarget:  estce que tu aimes moscou\n",
            "Epoch: 24  loss: 6.323659770190716\n",
            "\tOutput:  avezvous astu une copie\n",
            "\tTarget:  en avezvous une copie\n",
            "Epoch: 25  loss: 6.211436957120895\n",
            "\tOutput:  vous es le plus\n",
            "\tTarget:  tu es la doyenne\n",
            "Epoch: 26  loss: 6.230649124830961\n",
            "\tOutput:  vous ne pouvez pas arr notre d faite\n",
            "\tTarget:  vous ne pouvez pas admettre votre d faite\n",
            "Epoch: 27  loss: 6.213777665048838\n",
            "\tOutput:  qui est la personne\n",
            "\tTarget:  qui est cette personne\n",
            "Epoch: 28  loss: 6.147438123822212\n",
            "\tOutput:  en avezvous assez\n",
            "\tTarget:  en avezvous assez\n",
            "Epoch: 29  loss: 6.0999782755970955\n",
            "\tOutput:  vous ne pouvez pas <eos> faire\n",
            "\tTarget:  vous ne pouvez pas le manquer\n",
            "Epoch: 30  loss: 6.128920000046492\n",
            "\tOutput:  il peux mieux de venir\n",
            "\tTarget:  tu ferais mieux de venir\n",
            "Epoch: 31  loss: 6.062776416540146\n",
            "\tOutput:  tu astu fini t <eos>\n",
            "\tTarget:  en avezvous bient t termin\n",
            "Epoch: 32  loss: 6.018588397651911\n",
            "\tOutput:  ton livre est ici\n",
            "\tTarget:  ton livre est ici\n",
            "Epoch: 33  loss: 6.026545863598585\n",
            "\tOutput:  ne le pas le chien\n",
            "\tTarget:  ne nourris pas le chien\n",
            "Epoch: 34  loss: 6.000821793451905\n",
            "\tOutput:  j'aimerais aime le <eos> le monde\n",
            "\tTarget:  vous aimez bien tout le monde\n",
            "Epoch: 35  loss: 5.970712458714843\n",
            "\tOutput:  avezvous avez aije f\n",
            "\tTarget:  vous prenez la visa\n",
            "Epoch: 36  loss: 5.943024851381779\n",
            "\tOutput:  ne faites appr personne\n",
            "\tTarget:  ne vous fiez quiconque\n",
            "Epoch: 37  loss: 5.9201370272785425\n",
            "\tOutput:  aimestu l' cole\n",
            "\tTarget:  aimestu l' cole\n",
            "Epoch: 38  loss: 5.866560077294707\n",
            "\tOutput:  pourquoi astu fait\n",
            "\tTarget:  pourquoi l'avezvous fait\n",
            "Epoch: 39  loss: 5.833182640373707\n",
            "\tOutput:  vous tes tes <eos>\n",
            "\tTarget:  vous vous exprimez bien\n",
            "Epoch: 40  loss: 5.854692164808512\n",
            "\tOutput:  pourquoi sontils l\n",
            "\tTarget:  pourquoi sontils ici\n",
            "Epoch: 41  loss: 5.8233438935130835\n",
            "\tOutput:  tesvous catholiques\n",
            "\tTarget:  tesvous courageuses\n",
            "Epoch: 42  loss: 5.8179757706820965\n",
            "\tOutput:  je faits perdu la partie\n",
            "\tTarget:  tu as perdu la partie\n",
            "Epoch: 43  loss: 5.768715085461736\n",
            "\tOutput:  tom aime mary\n",
            "\tTarget:  tom aimetil mary\n",
            "Epoch: 44  loss: 5.792667334899306\n",
            "\tOutput:  vous tes fort s courageux\n",
            "\tTarget:  vous tes tr s courageux\n",
            "Epoch: 45  loss: 5.712847234681249\n",
            "\tOutput:  tesvous tesvous s r\n",
            "\tTarget:  en tesvous s re\n",
            "Epoch: 46  loss: 5.7236022390425205\n",
            "\tOutput:  vous du du\n",
            "\tTarget:  payerastu en liquide\n",
            "Epoch: 47  loss: 5.677805442363024\n",
            "\tOutput:  aimestu que tu aimes les magasin\n",
            "\tTarget:  estce que tu aimes le saumon\n",
            "Epoch: 48  loss: 5.664185099303722\n",
            "\tOutput:  proc terai plait des <eos>\n",
            "\tTarget:  ach te bas vends haut\n",
            "Epoch: 49  loss: 5.5991493333131075\n",
            "\tOutput:  pourquoi monde\n",
            "\tTarget:  pourquoi riraisje\n",
            "Epoch: 50  loss: 5.569287244230509\n",
            "\tOutput:  a dans retard\n",
            "\tTarget:  tesvous en difficult\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Eaj1lEMhIB",
        "colab_type": "text"
      },
      "source": [
        "# Test your translator\n",
        "\n",
        "##### Unless you speak french you're going have to check it with google translate https://translate.google.com/\n",
        "##### I found it started doing alright once the loss got below 10 but this might take a while"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGZrs13eMhIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35b4c686-5fd8-438d-9eb6-c86b95509bff"
      },
      "source": [
        "sentence = 'how are you'\n",
        "translate(sentence)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comment vastu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-293KUMhII",
        "colab_type": "text"
      },
      "source": [
        "#### Test it on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEtYT0AVMhIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4cb62fda-dd4b-4f28-8f4b-da76aceda546"
      },
      "source": [
        "model.eval()\n",
        "scores = []\n",
        "preds = []\n",
        "targetlist = []\n",
        "with torch.no_grad():\n",
        "  for j,(context, target) in enumerate(testloader):\n",
        "          trg_input = target[:,:-1]\n",
        "          targets = target[:,1:].contiguous().view(-1)\n",
        "          targetlist.append(targets)\n",
        "          src,trg = mask(context,trg_input)\n",
        "          output = model(context,trg_input,src,trg)\n",
        "          pred = F.softmax(output,2).argmax(2)\n",
        "          preds.append(pred)\n",
        "          correct = sum(pred[0][targets!=fr_to_ix['<pad>']]==targets[targets!=fr_to_ix['<pad>']]).item()/len(targets[targets!=fr_to_ix['<pad>']])\n",
        "          scores.append(correct)\n",
        "plt.plot(scores)\n",
        "print('Average # of words correct',np.mean(scores))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average # of words correct 0.6180747835497835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYEmBCHIMhIP",
        "colab_type": "text"
      },
      "source": [
        "# MULTI-HEAD ATTENTION (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2ZPAbIfMhIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class multi_attention(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,dropout = .1):\n",
        "        super().__init__()\n",
        "        \n",
        "        heads = []\n",
        "        for i in range(???): ### TODO ### CHOOSE THE # OF HEADS YOU WANT\n",
        "            heads.append(???) ### TODO ### ADD SELF_ATTENTION LAYERS TO HEADS\n",
        "        \n",
        "        self.heads = nn.ModuleList(heads)\n",
        "        \n",
        "        self.linear = nn.Linear(???,encoder_dim) ### TODO ###\n",
        "    \n",
        "    \n",
        "    def forward(self,x,mask=None):\n",
        "        headoutputs = [layer(x,mask) for layer in self.heads]\n",
        "        headoutputs = torch.cat(headoutputs,dim=2)\n",
        "        return self.linear(headoutputs)\n",
        "    \n",
        "class encoder(nn.Module):\n",
        "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.enc_dim = enc_dim\n",
        "        self.attention = multi_attention(dim,enc_dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(enc_dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(enc_dim,enc_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.residual = nn.Linear(dim,enc_dim)\n",
        "        self.norm2 = nn.LayerNorm(enc_dim)\n",
        "    \n",
        "    def forward(self,x,mask):\n",
        "        z = self.attention(x,mask)\n",
        "        if self.dim != self.enc_dim:\n",
        "            x = self.residual(x)\n",
        "        z = self.norm1(x+z)\n",
        "        z2 = self.linear(z)\n",
        "        return self.norm2(z+z2)\n",
        "     \n",
        "    \n",
        "class decoder(nn.Module):\n",
        "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
        "        super().__init__()\n",
        "        self.attention = multi_attention(input_size,dim,dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.EDattention = encdec_attention(dim,dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "    \n",
        "    def forward(self,x,k,v,src,trg):\n",
        "        z = self.attention(x,trg)\n",
        "        z = self.norm1(z+x)\n",
        "        z2 = self.EDattention(z,k,v,src)\n",
        "        z2 = self.norm2(z2+z)\n",
        "        z3 = self.linear(z2)\n",
        "        return self.norm3(z3+z2)\n",
        "    \n",
        "class transformer(nn.Module):\n",
        "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
        "        super().__init__()\n",
        "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
        "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
        "        \n",
        "        self.pe1 = PositionalEncoder(dim,enmax)\n",
        "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
        "        self.encoders = []\n",
        "    \n",
        "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))   ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
        "        self.encoders = nn.ModuleList(self.encoders)\n",
        "        \n",
        "        self.decoders = []\n",
        "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
        "        self.decoders = nn.ModuleList(self.decoders)\n",
        "        \n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(encoder_dim,dec_vocab_size),\n",
        "            nn.LogSoftmax(2)\n",
        "        )\n",
        "        \n",
        "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
        "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
        "        \n",
        "    def create_dec_KV(self,z):\n",
        "        K = self.k(z)\n",
        "        V = self.v(z)\n",
        "        return K,V\n",
        "    \n",
        "    def encode(self,x,src):\n",
        "        x = self.embedding1(x)\n",
        "        x = self.pe1(x)\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x,src)\n",
        "        return x\n",
        "    \n",
        "    def decode(self,y,K,V,src,trg):\n",
        "        y = self.embedding2(y)\n",
        "        y = self.pe2(y)\n",
        "        for layer in self.decoders:\n",
        "            y = layer(y,K,V,src,trg)\n",
        "        return self.final(y)\n",
        "        \n",
        "    \n",
        "    def forward(self,x,y,src,trg):\n",
        "        x = self.encode(x,src)\n",
        "        K,V = self.create_dec_KV(x)\n",
        "        y = self.decode(y,K,V,src,trg)\n",
        "        \n",
        "        return y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9jaMau8MhIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDHx31_uMhIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YIHeaIWYMhIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for j,(context, target) in enumerate(trainloader):\n",
        "        trg_input = target[:,:-1]\n",
        "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
        "        targets = target[:,1:].contiguous().view(-1)\n",
        "        src,trg = mask(context,trg_input)\n",
        "        output = model(context,trg_input,src,trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    scheduler.step(total_loss)\n",
        "    print('Epoch:', i+1,' loss:', total_loss)\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    preds = []\n",
        "    targetlist = []\n",
        "    for j,(context, target) in enumerate(valloader):\n",
        "            trg_input = target[:,:-1]\n",
        "            targets = target.contiguous().view(-1)\n",
        "            targetlist.append(targets)\n",
        "            src,trg = mask(context,trg_input)\n",
        "            output = model(context,trg_input,src,trg)\n",
        "            pred = F.softmax(output,2).argmax(2)\n",
        "            preds.append(pred)\n",
        "            break\n",
        "    compareoutput(preds,targetlist,loc=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJXc0RrMhIo",
        "colab_type": "text"
      },
      "source": [
        "# Test your  multi-head transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a9sWHzEMhIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'how are you'\n",
        "translate(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBk9jCIaMhIu",
        "colab_type": "text"
      },
      "source": [
        "# QUESTION\n",
        "\n",
        "#### 1) Was the runtime of your multi-head transformer noticably longer than the single head one? What about the speed the loss decreased? If you had the time and resources to train it to a good spot, how did the translation quality compare to the single-headed transformer?\n",
        "\n",
        "#### 2)Try adding encoders and decoders to one of your transformers. Does having the extra layers improve performance? How does it affect runtime?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymrH2jVeMhIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}